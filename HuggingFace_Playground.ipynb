{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Huggingface Playground\n",
        "Pipelines, Models, Tokenizer\n",
        "Model Hub & Fine-tuning"
      ],
      "metadata": {
        "id": "Vy4lM5XakT07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "9EsXNokakOzn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9525d72-c251-4abf-a7ef-2ddb22ae1cd7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "z6C27B_4dnxK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentiment Analysis"
      ],
      "metadata": {
        "id": "7JJK8a6ivF3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "sentiment = classifier(\"I've eagerly anticipated the opportunity to explore a HuggingFace exercise notebook for as long as I can remember. I'm thrilled that I've finally had the chance to experience it.\")\n",
        "print(sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6CXDnGvvJrI",
        "outputId": "3941f4fc-677a-4408-ee78-d37882ef8746"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.9993579983711243}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Generation\n",
        "GPT2 Model (Default)"
      ],
      "metadata": {
        "id": "Q25YtNCqwc-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\"text-generation\")\n",
        "\n",
        "sentences = generator(\n",
        "    \"I just finished to bake a pizza, for the\",\n",
        "    max_length = 30,\n",
        "    num_return_sequences = 3,\n",
        ")\n",
        "\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1048MwRwg3R",
        "outputId": "36d98924-9f2d-4c92-a2cb-ae85924d03d6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'I just finished to bake a pizza, for the umpteenth time,\" said Michael Fiergill, 39, who owns the New York pizz'}, {'generated_text': 'I just finished to bake a pizza, for the first time today.\\nI love making pizza. I love it that much. I love it when'}, {'generated_text': 'I just finished to bake a pizza, for the price I was hoping someone would help. It turned out it really did.\\n\\nI started by'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenizer"
      ],
      "metadata": {
        "id": "Qs9_6ITcyxjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "sentence = \"I've eagerly anticipated the opportunity to explore a HuggingFace exercise notebook for as long as I can remember. I'm thrilled that I've finally had the chance to experience it.\"\n",
        "\n",
        "\n",
        "# Specify the model name. Here, I am using 'distilbert-base-uncased-finetuned-sst-2-english',\n",
        "# which is a version of DistilBERT that is fine-tuned for sentiment analysis (Default Model).\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "# Load the pre-trained model for sequence classification from Hugging Face\n",
        "# Using the from_pretrained function\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Load the tokenizer corresponding to the model.\n",
        "# This tokenizer will correctly process the input sentence for the model.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Create a pipeline for sentiment analysis. This pipeline will handle the process of\n",
        "# tokenizing the input sentence, feeding it into the model, and interpreting the output.\n",
        "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Use the classifier to analyze the sentiment of the sentence.\n",
        "res = classifier(sentence)\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpiOrRNryyTa",
        "outputId": "59b94015-1c16-4cc1-bd71-a6432c37bdbf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.9993579983711243}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " #Hugging Face Transformers library along with PyTorch for sentiment analysis"
      ],
      "metadata": {
        "id": "UAR_z0G9Fkq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n"
      ],
      "metadata": {
        "id": "rx0BiqYmzjHm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Specify the model name. Here, I am using 'distilbert-base-uncased-finetuned-sst-2-english',\n",
        "# which is a version of DistilBERT that is fine-tuned for sentiment analysis (Default Model).\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "X_train = [\"I am very happy today\", \"The moon is beautiful today\"]\n",
        "\n",
        "res = classifier(X_train)\n",
        "\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pT5_ZVceGPTi",
        "outputId": "dd885109-bdca-4e7d-ce7e-717bde06ecc9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.9998797178268433}, {'label': 'POSITIVE', 'score': 0.9998810291290283}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = tokenizer(X_train, padding=True, truncation=True, max_length=512,return_tensors=\"pt\")\n",
        "print(batch)\n",
        "\n",
        "with torch.no_grad():\n",
        "  outputs = model(**batch)\n",
        "  print(outputs)\n",
        "  predictions = F.softmax(outputs.logits, dim=1)\n",
        "  print(predictions)\n",
        "  labels = torch.argmax(outputs.logits, dim=1)\n",
        "  print(labels)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3TkO5eIGtNx",
        "outputId": "a2dff9d9-ba00-4315-92c7-2875ba86e190"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[ 101, 1045, 2572, 2200, 3407, 2651,  102],\n",
            "        [ 101, 1996, 4231, 2003, 3376, 2651,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1]])}\n",
            "SequenceClassifierOutput(loss=None, logits=tensor([[-4.3394,  4.6865],\n",
            "        [-4.3431,  4.6939]]), hidden_states=None, attentions=None)\n",
            "tensor([[1.2024e-04, 9.9988e-01],\n",
            "        [1.1891e-04, 9.9988e-01]])\n",
            "tensor([1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " #Hugging Face Transformers library along with PyTorch for Text-Generation (Language Modeling)"
      ],
      "metadata": {
        "id": "1chqSIfYSS4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Text generation setup\n",
        "prompt = \"Once upon a time\"\n",
        "max_length = 100\n",
        "\n",
        "# Encode context the generation is conditioned on\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "attention_mask = torch.ones(input_ids.shape, dtype=torch.long)  # Create an attention mask\n",
        "\n",
        "# Generate text\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=tokenizer.eos_token_id  # Explicitly set the pad token ID\n",
        "    )\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aaQUz4VH7f7",
        "outputId": "f9e263e5-bbeb-40df-d8eb-1d52aee6bad3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"emotion\")\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
        "# Tokenize the dataset using the tokenizer using BertTokenizer.\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "train_dataset = tokenized_datasets['train'].shuffle(seed=50).select(range(1000))  # Reduce dataset size for my PlayGround\n",
        "eval_dataset = tokenized_datasets['validation'].shuffle(seed=50).select(range(500))\n",
        "\n",
        "# Load a pre-trained BERT model, which I will fine-tune.\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)  # 6 labels in the 'emotion' dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIdi3hvFRPIR",
        "outputId": "27a7a636-d152-4d1d-dc9d-1325bdf744fb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the training configurations.\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # Output directory for model checkpoints\n",
        "    num_train_epochs=3,              # Total number of training epochs\n",
        "    per_device_train_batch_size=8,  # Batch size per device during training\n",
        "    per_device_eval_batch_size=32,   # Batch size for evaluation\n",
        "    warmup_steps=500,                # Number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # Strength of weight decay\n",
        "    logging_dir='./logs',            # Directory for storing logs\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",     # Evaluation is done at the end of each epoch\n",
        ")\n",
        "\n",
        "#Create a Trainer to handle the training.\n",
        "trainer = Trainer(\n",
        "    model=model,                         # The instantiated Transformers model to be trained\n",
        "    args=training_args,                  # Training arguments\n",
        "    train_dataset=train_dataset,         # Training dataset\n",
        "    eval_dataset=eval_dataset            # Evaluation dataset\n",
        ")\n"
      ],
      "metadata": {
        "id": "xqei3j4UVHro"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "GRBfWVW4VM5c",
        "outputId": "535be566-5c39-452d-aba4-f9bc2cd31f9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 20/375 14:48 < 4:52:08, 0.02 it/s, Epoch 0.15/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='21' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 21/375 15:37 < 4:51:07, 0.02 it/s, Epoch 0.16/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6jgCyyVea4Wb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}